{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST for Tensorflow-lite\n",
    "----\n",
    "\n",
    "This notebook is part of this [post](https://www.stupid-projects.com/machine-learning-on-embedded-part-3) which is part a series of post about using ML and NN in embedded MCUs. The first post of the series is [here](https://www.stupid-projects.com/machine-learning-on-embedded-part-1)\n",
    "\n",
    "This notebook is just a port of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook from Keras to TF.\n",
    "\n",
    "This notebook is meant to be used to train the MNIST NN and then export the model to TF-Lite for microcontrollers and uploaded to a stm32f746. Later there's a cell in the notebook that you can hand-draw a number on a window and then evaluate the model on both the notebook and the stm32f746 by running the inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "As it's mentioned before, this is just a port from Keras to TF of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook. For the model training we're going to use `convnets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train and test data\n",
    "Normally when the dataset is loaded the shape is (x, 28, 28). For convnets you need to reshape the data to (x, 28, 28, y), where `x` is the number of images per set and `y` in this case is the number of colors. Normally, of RGB it should be 3, but since the images are grayscale then it's 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.ndarray'>\n",
      "Dataset shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type:\", type(train_images))\n",
    "print(\"Dataset shape:\", (train_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 60000\n",
      "Possible values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", len(train_labels))\n",
    "print(\"Possible values:\", np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a digit from the dataset\n",
    "Now we just print a digit from the dataset in order to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.figure()\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Classification label: {}\".format(train_labels[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAEKCAYAAACsfbhjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAboklEQVR4nO3dfZxdVX3v8c+XQKQC5cEo0BAI2IhGCoMEsEIhXAgNXgWjyE1oEVpKsJf0+kiLXi9GfMGNCnihpOAAkYdXFKgK5NJooDwYrBIzQIAkNJcQKQyZFzFCIDwVQ373j70HTmbm7PMwZ+bsNfm+X6/zmrP3b++1V07O/GattfdeWxGBmVlKtml3BczMGuXEZWbJceIys+Q4cZlZcpy4zCw5TlxmlhwnLjMbMpLmSVonaXmVuCRdLmm1pEclfaiecp24zGwoXQdMLYifAEzIXzOBK+sp1InLzIZMRCwGni/Y5CTghsg8AOwiac9a5W7bqgrWQ5Iv0zcbYhGhwew/derUWL9+fV3bPvjggyuA1ytWdUZEZwOHGws8U7Hcna/rKdppUIlL0lTgMmAUcE1EzBlMeWbWfuvXr6erq6uubSW9HhGTBnG4gZJszQZO011FSaOAuWR91InADEkTmy3PzMojIup6tUA3MK5ieS9gba2dBjPGdRiwOiLWRMQbwE1k/VUzS9zmzZvrerXAAuAz+dnFDwMvRkRhNxEG11UcqG96eN+NJM0kO1tgZgloYWsKST8EJgNjJHUDXwe2y49zFbAQ+CiwGngV+Kt6yh1M4qqrb5oP1HWCB+fNUtGqxBURM2rEAzin0XIHk7ia6puaWfmVfZ6+wYxxLQUmSNpX0mhgOll/1cwSN4yD801pusUVEZskzQIWkV0OMS8iVrSsZmbWNmVvcQ3qOq6IWEg2uGZmI0REtOqM4ZAZ1ivnzSwNI7rFZWYjkxOXmSXHicvMktLuM4b1cOIys348OG9myXGLy8yS4q6imSXJicvMkuPEZWbJceIys6T4lh8zS5JbXGaWHCcuM0uOE5eZJceJy8yS4sF5M0uSW1xmlhwnLjNLjhOXmSXFN1mbWZKcuMwsOT6raGbJcYvLzJLiMS4zS5ITl5klx4nLzJLjxGVmSfG9imaWJLe4rK1GjRpVGN95552H9PizZs2qGnvnO99ZuO/+++9fGD/nnHMK4xdffHHV2IwZMwr3ff311wvjc+bMKYx/4xvfKIyX3YhOXJKeAjYCbwKbImJSKyplZu1V9sS1TQvKOCYiOpy0zEaO3mu5ar3qIWmqpFWSVks6b4D43pLulfSwpEclfbRWme4qmtkWWjk4L2kUMBeYAnQDSyUtiIiVFZt9DbglIq6UNBFYCIwvKnewLa4A7pT0oKSZVSo+U1KXpK5BHsvMhkkLW1yHAasjYk1EvAHcBJzU93DAH+bvdwbW1ip0sC2uIyJiraT3AHdJ+veIWLxFjSI6gU4ASeXuOJsZ0NAY15g+jZLO/He+11jgmYrlbuDwPmXMJmsA/R2wA3BcrYMOKnFFxNr85zpJt5Jl18XFe5lZ2TWQuNbXGN/WQMX3WZ4BXBcRl0j6U+BGSQdERNX+atNdRUk7SNqp9z1wPLC82fLMrBzq7SbWmdy6gXEVy3vRvyt4JnBLfuxfAdsDY4oKHUyLa3fgVkm95fwgIn42iPJGrL333rswPnr06ML4Rz7ykcL4kUceWTW2yy67FO77qU99qjDeTt3d3YXxyy+/vDA+bdq0qrGNGzcW7vvII48Uxn/+858XxlPXwsshlgITJO0LPAtMB07ts83TwLHAdZI+QJa4fltUaNOJKyLWAAc1u7+ZlVerzipGxCZJs4BFwChgXkSskHQB0BURC4AvAVdL+gJZN/KMqJE5fTmEmfXTygtQI2Ih2SUOlevOr3i/EjiikTKduMxsC55I0MyS5MRlZslx4jKz5DhxbQU6OjoK4/fcc09hfKinlimrWmeuvva1rxXGX3755cL4/Pnzq8Z6enoK933hhRcK46tWrSqMp8wTCZpZktziMrPkOHGZWXKcuMwsOU5cZpYUD86bWZLc4jKz5DhxbQWefvrpwvjvfve7wniZr+NasmRJYXzDhg2F8WOOOaZq7I033ijc98YbbyyM29Bx4jKzpPgmazNLkhOXmSXHZxXNLDlucZlZUjzGZWZJcuIys+Q4cW0Fnn/++cL4ueeeWxj/2Mc+Vhh/+OGHC+O1HtNVZNmyZYXxKVOmFMZfeeWVwvgHP/jBqrHPfe5zhfta+zhxmVlSfK+imSXJLS4zS44Tl5klx4nLzJLjxGVmSfHgvJklyS0u47bbbiuM13ru4saNGwvjBx10UNXYmWeeWbjvxRdfXBivdZ1WLStWrKgamzlz5qDKtqFT9sS1Ta0NJM2TtE7S8op1u0m6S9IT+c9dh7aaZjaceu9XrPVql5qJC7gOmNpn3XnA3RExAbg7XzazEaDepFXqxBURi4G+97ScBFyfv78e+ESL62VmbVT2xNXsGNfuEdEDEBE9kt5TbUNJMwEPZpglZKs/qxgRnUAngKRyj/iZWdtbU/WoZ4xrIM9J2hMg/7mudVUys3ZrZVdR0lRJqyStljTgeLikUyStlLRC0g9qldls4loAnJ6/Px24vclyzKyEWpW4JI0C5gInABOBGZIm9tlmAvAV4IiI+CDw+Vrl1uwqSvohMBkYI6kb+DowB7hF0pnA08Cna/4LrKqXXnppUPu/+OKLTe971llnFcZvvvnmwnjZx0KsOS3sKh4GrI6INQCSbiI7ubeyYpuzgLkR8UJ+7Jo9uJqJKyJmVAkdW2tfM0tPg7f8jJHUVbHcmY9r9xoLPFOx3A0c3qeM9wFI+jdgFDA7In5WdFBfOW9m/TTQ4lofEZMK4hqo+D7L2wITyHp2ewH3SzogIqo+Jr3ZMS4zG8FaODjfDYyrWN4LWDvANrdHxO8j4jfAKrJEVpUTl5n108LEtRSYIGlfSaOB6WQn9yrdBhwDIGkMWddxTVGh7iqaWT+tGpyPiE2SZgGLyMav5kXECkkXAF0RsSCPHS9pJfAmcG5E/K6oXCcuM9tCqy9AjYiFwMI+686veB/AF/NXXZy4RoDZs2dXjR1yyCGF+x599NGF8eOOO64wfueddxbGLU1lv8zFicvM+in7LT9OXGbWjxOXmSUlhZusnbjMrB8nLjNLjhOXmSXHZxXNLCke47JhUfQIsVrT1jz00EOF8auvvrowfu+99xbGu7q6qsbmzp1buG/Zf3lGsrJ/9k5cZtaPE5eZJceJy8yS0uBEgm3hxGVm/bjFZWbJceIys+Q4cZlZcpy4rK2efPLJwvgZZ5xRGP/+979fGD/ttNOaju+www6F+95www2F8Z6ensK4NccXoJpZknxW0cyS4xaXmSXHicvMkuIxLjNLkhOXmSXHicvMkuOzilZqt956a2H8iSeeKIxfeumlhfFjjz22auyiiy4q3HefffYpjF944YWF8WeffbYwbgNLYYxrm1obSJonaZ2k5RXrZkt6VtKy/PXRoa2mmQ2n3uRV69UuNRMXcB0wdYD1342Ijvy1cIC4mSWq7ImrZlcxIhZLGj/0VTGzski+q1hglqRH867krtU2kjRTUpek6pOPm1lp9E4kWM+rXZpNXFcC7wU6gB7gkmobRkRnREyKiElNHsvMhlnyXcWBRMRzve8lXQ3c0bIamVnbjciuoqQ9KxanAcurbWtm6Um+xSXph8BkYIykbuDrwGRJHUAATwFnD2EdrY2WLy/+m3TKKacUxj/+8Y9XjdWa6+vss4u/VhMmTCiMT5kypTBu1ZW9xVXPWcUZA6y+dgjqYmYl0O7WVD185byZ9VP2W34GczmEmY1QrRzjkjRV0ipJqyWdV7DdyZJCUs0rEJy4zKyfViUuSaOAucAJwERghqSJA2y3E/A/gCX11M+Jy8y2UG/SqrPFdRiwOiLWRMQbwE3ASQNs903g28Dr9RTqxGVm/TSQuMb03hmTv2b2KWos8EzFcne+7i2SDgbGRUTd14N6cN4GZcOGDYXxG2+8sWrsmmuuKdx3222Lv55HHXVUYXzy5MlVY/fdd1/hvlu7Bs4qrq9xV4wGKv6toLQN8F3gjLorhxOXmQ2ghWcVu4FxFct7AWsrlncCDgDukwSwB7BA0okRUfX+ZicuM9tCi6/jWgpMkLQv8CwwHTi14lgvAmN6lyXdB3y5KGmBx7jMbACtGpyPiE3ALGAR8DhwS0SskHSBpBObrZ9bXGbWTyuvnM8nGl3YZ935VbadXE+ZTlxm1o9v+TGzpPROJFhmTlxm1o9bXJa0Aw88sDB+8sknF8YPPfTQqrFa12nVsnLlysL44sWLB1X+1syJy8yS48RlZslx4jKzpHgiQTNLks8qmlly3OIys+Q4cZlZUjzGZW23//77F8ZnzZpVGP/kJz9ZGN9jjz0arlO93nzzzcJ4T09PYbzs4zRl5sRlZskpe9J34jKzLbiraGZJcuIys+Q4cZlZcpy4zCw5TlxmlpQRMZGgpHHADWSPDdoMdEbEZZJ2A24GxgNPAadExAtDV9WtV61rpWbMmFE1Vus6rfHjxzdTpZbo6ip8kAsXXnhhYXzBggWtrI5VKHuLq56n/GwCvhQRHwA+DJwjaSJwHnB3REwA7s6XzWwEaNVTfoZKzcQVET0R8VD+fiPZI4bGAicB1+ebXQ98YqgqaWbDq+yJq6ExLknjgYOBJcDuEdEDWXKT9J6W187Mhl27k1I96k5cknYEfgx8PiJeyh+XXc9+M4GZzVXPzNphRCQuSduRJa35EfGTfPVzkvbMW1t7AusG2jciOoHOvJxyfxpmBpT/XsWaY1zKmlbXAo9HxKUVoQXA6fn704HbW189M2uHkTDGdQRwGvCYpGX5uq8Cc4BbJJ0JPA18emiqmL7dd9+9MD5x4sTC+BVXXFEYf//7399wnVplyZIlhfHvfOc7VWO33178t67sf/VHqnYnpXrUTFwR8Qug2oDWsa2tjpmVQfKJy8y2Pk5cZpacsnfTnbjMbAsjYozLzLY+TlxmlhwnLjNLjhPXCLHbbrtVjX3ve98r3Lejo6Mwvt9++zVVp1b45S9/WRi/5JJLCuOLFi0qjL/22msN18nar5WJS9JU4DJgFHBNRMzpE/8i8DdkM9H8FvjriPiPojLrmdbGzLYivRMJ1vOqRdIoYC5wAjARmJFPi1XpYWBSRBwI/Aj4dq1ynbjMrJ8W3vJzGLA6ItZExBvATWRTYlUe696IeDVffADYq1ah7iqaWT8NdBXHSKqcyrYzn1ih11jgmYrlbuDwgvLOBH5a66BOXGbWTwOJa31ETCqID3S74ICFS/pLYBJwdK2DOnGZ2RZafAFqNzCuYnkvYG3fjSQdB/xP4OiI+M9ahTpxmVk/LUxcS4EJkvYFngWmA6dWbiDpYOB7wNSIGHBev76cuMysn1bdqxgRmyTNAhaRXQ4xLyJWSLoA6IqIBcB3gB2Bf85nVn46Ik4sKnerSVyHH140HgjnnntuYfywww6rGhs7dmxTdWqVV199tWrs8ssvL9z3oosuKoy/8sorTdXJ0tbK67giYiGwsM+68yveH9domVtN4jKz+vgmazNLkhOXmSXHicvMkuOJBM0sKR7jMrMkOXGZWXKcuEpi2rRpg4oPxsqVKwvjd9xxR2F806ZNhfGiObM2bNhQuK/ZQJy4zCw5TlxmlpTeiQTLzInLzPpxi8vMkuPEZWbJceIys6T4AlQzS1LZE5dqVVDSOOAGYA9gM9lk+JdJmg2cRfYcNICv5vPuFJVV7k/DbASIiIHmea/b6NGj493vfndd265du/bBGnPOD4l6WlybgC9FxEOSdgIelHRXHvtuRFw8dNUzs3Yoe4urZuKKiB6gJ3+/UdLjZI8cMrMRKIUxroYeCCtpPHAwsCRfNUvSo5LmSdq1yj4zJXX1efaamZVYCx8IOyTqTlySdgR+DHw+Il4CrgTeC3SQtcgGvGEuIjojYlI7+sFm1pyyJ666zipK2o4sac2PiJ8ARMRzFfGrgeI7hc0sGWW/5admi0vZ84KuBR6PiEsr1u9Zsdk0YHnrq2dmw63e1lbZW1xHAKcBj0lalq/7KjBDUgfZ47SfAs4ekhqa2bAr++B8PWcVfwEMdF1I4TVbZpau5BOXmW19nLjMLDlOXGaWFE8kaGZJcovLzJLjxGVmyXHiMrOktPvi0no4cZlZP05cZpYcn1U0s+S4xWVmSUlhjKuhiQTNbOvQytkhJE2VtErSaknnDRB/h6Sb8/iSfMLSQk5cZtZPqxKXpFHAXOAEYCLZrDIT+2x2JvBCRPwx8F3gW7XKdeIys342b95c16sOhwGrI2JNRLwB3ASc1Gebk4Dr8/c/Ao7N5wGsarjHuNYD/1GxPCZfV0ZlrVtZ6wWuW7NaWbd9WlDGIrI61WP7Ps+T6IyIzorlscAzFcvdwOF9ynhrm4jYJOlF4F0UfCbDmrgiYouHtUnqKutc9GWtW1nrBa5bs8pWt4iY2sLiBmo59e1j1rPNFtxVNLOh1A2Mq1jeC1hbbRtJ2wI7A88XFerEZWZDaSkwQdK+kkYD04EFfbZZAJyevz8ZuCdqjPy3+zquztqbtE1Z61bWeoHr1qwy121Q8jGrWWTjZqOAeRGxQtIFQFdELCB7GM+NklaTtbSm1ypXZb/QzMysL3cVzSw5Tlxmlpy2JK5atwC0k6SnJD0maVmf61PaUZd5ktZJWl6xbjdJd0l6Iv+5a4nqNlvSs/lnt0zSR9tUt3GS7pX0uKQVkj6Xr2/rZ1dQr1J8bikZ9jGu/BaA/wdMITsNuhSYERErh7UiVUh6CpgUEW2/WFHSUcDLwA0RcUC+7tvA8xExJ0/6u0bEP5SkbrOBlyPi4uGuT5+67QnsGREPSdoJeBD4BHAGbfzsCup1CiX43FLSjhZXPbcAGBARi+l/PUvl7RHXk33xh12VupVCRPRExEP5+43A42RXZ7f1syuolzWoHYlroFsAyvSfF8Cdkh6UNLPdlRnA7hHRA9kvAvCeNtenr1mSHs27km3pxlbKZxo4GFhCiT67PvWCkn1uZdeOxNXw5f3D7IiI+BDZ3ezn5F0iq8+VwHuBDqAHuKSdlZG0I/Bj4PMR8VI761JpgHqV6nNLQTsSVz23ALRNRKzNf64DbiXr2pbJc/lYSe+Yybo21+ctEfFcRLwZEZuBq2njZydpO7LkMD8ifpKvbvtnN1C9yvS5paIdiaueWwDaQtIO+aApknYAjgeWF+817CpvjzgduL2NddlCb1LITaNNn10+Jcq1wOMRcWlFqK2fXbV6leVzS0lbrpzPT/f+H96+BeDCYa/EACTtR9bKgux2qB+0s26SfghMJpti5Dng68BtwC3A3sDTwKcjYtgHyavUbTJZdyeAp4Cze8eUhrluRwL3A48BvZNGfZVsPKltn11BvWZQgs8tJb7lx8yS4yvnzSw5TlxmlhwnLjNLjhOXmSXHicvMkuPEZWbJGZLEJWkPSTdJelLSSkkLJb1P0vjKaVBacJwLJB2Xv/+zfKqQZZLGSvpRk2WeIemPKpavUf8HWDZb7hU1tpkt6csNlvtyHdv0TtUzKV/eV9kTg59Q9gTh0XWUcXq+/ROSTq9j+4afTizpkLyeqyVdnl+wWWufr+Tbr5L053Vs/+n8e7K59/MoSb0annJHdUwPJekLkp6u9d1LTr1PrG3gybYCfgV8tmJdB/BnwHhgeauPmR/jKuCvWlDOfWTT2rS6fmcAV9TYZjbw5QbLfbmObZ4CxlQs3wJMr/jc/rbG/rsBa/Kfu+bvd62xz38HrsrfTwdurqOevwb+NP8O/RQ4ocb2E4FHgHcA+wJPAqNq7PMBYP9G/p+HqV7fBs7L358HfKvG9qPycvcDRufHm9jsdy+111C0uI4Bfh8RV/WuiIhlEXF/5UZ56+t+SQ/lr4/k6/eUtDhvOS3PW1KjJF2XLz8m6Qv5ttdJOlnS35DNaXS+pPmVLbt834vz/R6V9Hf5+vMlLc3L7FTmZGASMD8//h9Iuq+ipTIjL2e5pG9V/FtelnShpEckPSBp96IPSNLH81bIw5L+tc/2B0m6J//Le1bFPufm9X1U0jea+Y/JyxHwX8ieGAz1Te/y58BdEfF8RLwA3AXUevZeQ08nVnbbyx9GxK8i+227oY56nQTcFBH/GRG/AVZT4z6/iHg8IlbVKHfY60XjU+5s1dNDDUXiOoBsgrRa1gFTIpuJ4b8Bl+frTwUWRUQHcBCwjKzFNjYiDoiIPwG+X1lQRFxDdh/auRHxF32OM5Psr97BEXEgMD9ff0VEHBrZJHh/AHwsIn4EdAF/EREdEfFabyF59/FbZL/0HcChknq/XDsAD0TEQcBi4K2EU8UvgA9HxMFkX7i/r4gdCPxXsr/w50v6I0nHAxPIvqwdwCEaYNYKSctqHBeyJwRviIhN+XI90wo1MxXRFk8nBnqfTly0fXezx2hgn0YNV70anXKn7NNDDal2Pp5sO+AKSR3Am8D78vVLgXnK7qK/LSKWSVoD7CfpH4F/Ae5s4DjHkXVZNgHE2/emHSPp74F3knWBVgD/t6CcQ4H7IuK3AJLmA0eR3Tv4BnBHvt2DZLO7FtkLuDn/az4a+E1F7PY8Yb4m6V6yZHUk2Q3fD+fb7EiWyBZXFpon+1qamVZoOPYZrno1aiTVa8QYihbXCuCQOrb7AtnNuQeRdc9Gw1szax4FPEv2rLXP5N2Tg8jGJc4BrmmgPqLPf6ik7YF/Ak7OW3BXA9vXUU41v8+7EZAl4Vp/EP6RrMX3J8DZfY7d98sX+bH/d94K7IiIP46Ia2sco5r1wC7KnhgM9U0r1MxURI0+nbg7L7epYzSwT6OGq16NTrlT6umhhtpQJK57gHf0GZ85VNLRfbbbGeiJbA6i08gGG5G0D7AuIq4mmwLkQ5LGANtExI+B/wV8qIH63Al8tvcXVdJuvJ0o1iub1O3kiu03AjsNUM4S4GhJY5TNmz8D+HkD9ai0M1lihrenWel1kqTtJb2LbLaFpWQP0/zrvK4oO2va1OydeYK9l7f/zfVM77IIOF7SrvnZruPzdUUaejpx3j3aKOnD+VjYZ+qo1wJgurIzmPuStUJ/XWOfhgxjvRqdcqe000MNh5YnrvzLOQ2YouxyiBVkZ8v6/jX4J+B0SQ+QdRNfyddPBpZJehj4FHAZWd/9vnwM5zrgKw1U6RqyKUwelfQIcGpEbCBrZT1G1tVbWrH9dcBVvYPzFf+unvy495KdwXkoIpqdz2k28M+S7idrAVX6NVl3+AHgmxGxNiLuBH4A/ErSY2SD3f2Sa51jXAD/AHxR2ZOD30X2BwJJJyp7wvAW8u71N8k+p6XABb1dbmWXiwx0WcG1wLvyY3yR7EwZ+Zjdwir1+luy/6/VZGfMfprv81lJnx2gXivIzpCuBH4GnBMRb+b7LFTFZS29JE2T1E02hvgvkhaVoV7AHLLfmSfIhhrm5NtPktSvh5EPffQ+Ifpx4Jb8uL2XCZ1Y5d8yInham62ASvTkIht+ks4g+/+f1e66tIqvnN86/Ba4u0rLyEYwZZcOfQUozZz7reAWl5klxy0uM0uOE5eZJceJy8yS48RlZsn5/22PrWk3rMklAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.1825 - acc: 0.9433\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0492 - acc: 0.9846\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.0340 - acc: 0.9896\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0251 - acc: 0.9922\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0200 - acc: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f55d977c630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0297 - acc: 0.9911\n",
      "Test accuracy: 0.9911\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0297 - acc: 0.9911\n",
      "Restored model, accuracy: 99.11%\n",
      "Restored model, loss: 0.029690922645915815\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "print(\"Restored model, loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to tflite\n",
    "\n",
    "Now we need to export the model and save it in a `h5` file. Then we use the `TFLiteConverter` to convert the model to the flatbuffer tflite format.\n",
    "\n",
    "Normally, we should use quantization on the model as it's explained [here](https://www.tensorflow.org/lite/microcontrollers/build_convert#quantization), but for some reason in the current version I'm using (1.14) that doesn't work and when the model is loaded on the stm32f746, then I get this error:\n",
    "\n",
    "```\n",
    "Only float32, int16, int32, int64, uint8, bool, complex64 supported currently\n",
    "```\n",
    "\n",
    "This error comes from the `source/libs/tensorflow/lite/experimental/micro/simple_tensor_allocator.cc` file and the reason is that when the model is converted with `TFLiteConverter`, then the output is set to `kTfLiteInt8`, which means signed integer and that is not yet supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you want to add post-quantization during conversion (which doesn't work yet), then you need to uncomment the line in the next code. Finally, the output of the next command is the size of the flatbuffer model in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 14:33:44.494164 140008927770432 deprecation.py:506] From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0715 14:33:44.495206 140008927770432 deprecation.py:506] From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0715 14:33:44.941275 140008927770432 deprecation.py:323] From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0715 14:33:44.942090 140008927770432 deprecation.py:323] From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the converted flatbuffer is: 375740 bytes\n"
     ]
    }
   ],
   "source": [
    "tflite_mnist_model = 'mnist.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('mnist_keras.h5')\n",
    "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "flatbuffer_size = open(tflite_mnist_model, \"wb\").write(tflite_model)\n",
    "\n",
    "print('The size of the converted flatbuffer is: %d bytes' % flatbuffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a header file from the tflite model\n",
    "Now that you have your tflite flatbuffer you can convert it to a header file\n",
    "in order to add it to your C++ code you need to run this command in bash in\n",
    "the `jupyter _notebook` folder.\n",
    "\n",
    "```sh\n",
    "xxd -i mnist-quant.tflite > model_data.h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate the model\n",
    "\n",
    "In order to make it more interesting, I've wrote a small python that you can draw a digit with your mouse and then ran the prediction function to evaluate the result. For this purpose I'm using tkinter and the PIL library. Therefore you need to install them in your environment.\n",
    "\n",
    "For ubuntu (I'm using conda):\n",
    "```sh\n",
    "sudo apt install python3-tk\n",
    "conda install Pillow\n",
    "```\n",
    "\n",
    "#### How to use:\n",
    "Run the following two cells and this window will show up.\n",
    "\n",
    "![Image](./stm32comms/digit_draw_1.png)\n",
    "\n",
    "In the left window you can draw any digit with your mouse by clicking in the white area. Just be sure that you don't draw that too fast because then you get dotted lines. Then you can either press one of the following buttons:\n",
    "* `Clear`: clears the input drawing area\n",
    "* `Export`: Converts the draw digit to the MNIST input format and then exports the digit to a file called `digit.txt`. You can use this file in this notebook and evaluate the result.\n",
    "* `Inference`: Converts the digit to the MNIST input format and sends the data to the MCU via the serial port. Then the MCU runs the prediction and returns an array with the output values and the time that spend for the calculation.\n",
    "\n",
    "This is an example (I'm right-handed but I'm the mouse with my left hand, this is why it seems so ugly, lol).\n",
    "\n",
    "![Image](./stm32comms/digit_draw_2.png)\n",
    "\n",
    "\n",
    "Anyway, try yourself by running the next two cells.\n",
    "\n",
    "> Warning: If you proceed with the export function and local evaluation, then you need first to terminate the tkinter window, because the notebook is not able to run 2 cells at the same time. Therefore, if the window thread is running then no other cell can be run.\n",
    "\n",
    "> Note: Be carefull that if you do any changes in any python class or script that is already loaded from the jupyter notebook kernel, then you need to restart the kernel (File menu: Kernel-> Restart). Otherwise the previous loaded class will be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *\n",
    "from stm32comms.MnistDigitDraw import MnistDigitDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the followinf cell how many times you like in order to draw a new digit every time. When you do, then press the `Inference` button, then the `Export` and then close the window to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not implemented yet...\n",
      "Not implemented yet...\n",
      "Exported image to digit.txt\n"
     ]
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.title(\"MNIST digit draw\")\n",
    "d = MnistDigitDraw(root, 250, 250)\n",
    "d.start()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate hand-written digit on the notebook\n",
    "\n",
    "In case you wnt to test your hand-written digit here then run the following cells. The next cells will load the `digit.txt` file which was exported in the previous step.\n",
    "\n",
    "The imported digit share is (768,). If you just open the file in a text editor (or cat the file) you'll see that each pixel value is one line. Therefore, after loading the digit then you need to convert from `(768,)` to `(1, 28, 28, 1)`, which is the format that the model prediction function support. To do that you need first to reshape the array to `(28, 28)` and then add two additional dimension in the tensor, one dimension in the beginning and one in the end. Those don't need to have a value, the first dimension is used as an index and the last as the prediction result, but it this case we don't care for any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load digit\n",
    "digit = np.loadtxt('digit.txt')\n",
    "# Reshape\n",
    "loaded_digit = digit.reshape(28,28)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=0)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=3)\n",
    "loaded_digit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the digit that you drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(loaded_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and interpreter\n",
    "\n",
    "To evaluate the hand-written digit in the notebook then you need to create an interpreter and then feed the image (or array) in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: conv2d_3_input\n",
      "shape: [ 1 28 28  1]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "name: dense_3/Softmax\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "DUMP INPUT\n",
      "{'name': 'conv2d_3_input', 'index': 3, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "\n",
      "DUMP OUTPUT\n",
      "{'name': 'dense_3/Softmax', 'index': 15, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_mnist_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\nDUMP INPUT\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"\\nDUMP OUTPUT\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results: [[2.58793570e-10 9.99998927e-01 2.15490488e-08 3.45071058e-14\n",
      "  1.24757005e-08 6.16702664e-07 3.13880406e-07 9.76472325e-09\n",
      "  6.71892906e-08 4.10876357e-13]]\n",
      "Predicted value: 1\n"
     ]
    }
   ],
   "source": [
    "loaded_digit = loaded_digit.astype('float32')\n",
    "input_details = interpreter.get_input_details()\n",
    "interpreter.set_tensor(input_details[0]['index'], loaded_digit)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results:\", output_data)\n",
    "print(\"Predicted value:\", np.argmax(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result the one which has the larger value is the predicted digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the STM32F746\n",
    "\n",
    "Now you can convert the digit and upload it to the stm32 for evaluation in the model that is running in the embedded board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes = digit.tobytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method tobytes of numpy.ndarray object at 0x7f5617c3b2b0>\n"
     ]
    }
   ],
   "source": [
    "print(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"bytes.txt\", bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f= open(\"digit.h\",\"w+\")\n",
    "f.write(\"const float f_digit[] = { \\n\")\n",
    "for d in digit:\n",
    "    f.write(str(d))\n",
    "    f.write(',')\n",
    "f.write('\\n};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
