{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST keras model\n",
    "\n",
    "This notebook is part of this [post](https://www.stupid-projects.com/machine-learning-on-embedded-part-1) which is part a series of post about using ML and NN in embedded MCUs.\n",
    "\n",
    "I've taken this notebook has been taken from this github repo and just added a few stuff:\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "## Additions\n",
    "You don't have to use all the notebook. The first part is only if you want to train again your model. Currently the trained model is already part of the stm32f746 firmware. In the last part I've added a small part of code that you can draw a digit and then send it to the stm32f746 and get back the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.1 - Introduction to convnets\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been \n",
    "through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its \n",
    "accuracy will still blow out of the water that of the densely-connected model from Chapter 2.\n",
    "\n",
    "The 6 lines of code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a \n",
    "minute what they do concretely.\n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n",
    "In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n",
    "passing the argument `input_shape=(28, 28, 1)` to our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n",
    "and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n",
    "the `Conv2D` layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n",
    "already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n",
    "So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n",
    "looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n",
    "\n",
    "Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter \n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dimtass/miniconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.1826 - acc: 0.9413\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0463 - acc: 0.9857\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0321 - acc: 0.9902\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0254 - acc: 0.9923\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0196 - acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3851e2def0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we \n",
    "decreased our error rate by 68% (relative). Not bad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_no_optimizer.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *\n",
    "from stm32comms.MnistDigitDraw import MnistDigitDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "d = MnistDigitDraw(root, 250, 250)\n",
    "d.start()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = np.loadtxt('stm32comms/digit2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = digit.reshape(28,28)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = np.expand_dims(x, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2229915e-09, 2.4238827e-06, 4.4001154e-06, 9.9604118e-01,\n",
       "        7.8605096e-09, 3.9505456e-03, 5.8800081e-10, 3.4948805e-08,\n",
       "        1.1089737e-06, 2.8340074e-07]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7969732e-14, 3.3321804e-11, 7.7299261e-15, 1.8763968e-06,\n",
       "        2.0775243e-14, 9.9999809e-01, 1.1126530e-10, 2.4182195e-12,\n",
       "        5.9399077e-09, 2.2940748e-08]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = train_images[0]\n",
    "num = np.expand_dims(num, axis=0)\n",
    "model.predict(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
